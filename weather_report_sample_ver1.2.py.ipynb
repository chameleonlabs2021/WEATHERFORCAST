{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36787e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/25 00:27:46 WARN Utils: Your hostname, rahul-System-Product-Name resolves to a loopback address: 127.0.1.1; using 192.168.10.100 instead (on interface wlp8s0)\n",
      "22/01/25 00:27:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/01/25 00:27:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# import dask.dataframe as dd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, col, udf\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "import pandas as pd\n",
    "# from dataprep.clean import validate_country\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b38b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/home/rahul/Interview/Weatherforcast/city_temperature_sample.csv\")\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/home/rahul/Interview/Weatherforcast/worldcities_sample.csv\")\n",
    "# df1.count()\n",
    "# df2.count()\n",
    "\n",
    "\n",
    "#Type cast day month and year to int \n",
    "df1=df1.withColumn('day',df1['day'].cast(\"int\").alias('day'))\n",
    "df1=df1.withColumn('month',df1['month'].cast(\"int\").alias('month'))\n",
    "df1=df1.withColumn('year',df1['year'].cast(\"int\").alias('year'))\n",
    "df1=df1.withColumn('AvgTemperature',df1['AvgTemperature'].cast(\"float\").alias('AvgTemperature'))\n",
    "df2=df2.withColumn('lat',df2['lat'].cast(\"float\").alias('lat'))\n",
    "df2=df2.withColumn('lng',df2['lng'].cast(\"float\").alias('lng'))\n",
    "\n",
    "df1.createOrReplaceTempView(\"city_temperature\")\n",
    "df2.createOrReplaceTempView(\"worldcities\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3e2373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "city_temperature1=spark.sql(\"\"\"\n",
    "select Region, country as temp_country, State, City as city_temp,\n",
    "case when day >31 or day <1 then null else day end as day1, day,case when month >12 or month <1 then null else month end as month1,\n",
    "month,year, case when length(year) <4 or length(year) >4 then null else year end year1,\n",
    "AvgTemperature from city_temperature \"\"\")\n",
    "\n",
    "city_temperature1.createOrReplaceTempView(\"city_temperature1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "481066a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----+---------+----+---+------+-----+----+-----+--------------+\n",
      "|Region|temp_country|State|city_temp|day1|day|month1|month|year|year1|AvgTemperature|\n",
      "+------+------------+-----+---------+----+---+------+-----+----+-----+--------------+\n",
      "|Africa|       Japan| null|  Algiers|   1|  1|     1|    1|1995| 1995|          64.2|\n",
      "|Africa|       Japan| null|  Algiers|   2|  2|     1|    1| 195| null|          49.4|\n",
      "|Africa|       Japan| null|  Algiers|  21| 21|     3|    3|null| null|          56.7|\n",
      "|Africa|       Japan| null|  Algiers|  22| 22|  null|   13|1995| 1995|          51.4|\n",
      "|Africa|       Japan| null|  Algiers|null| 32|     3|    3|1995| 1995|          51.6|\n",
      "|Africa|       Japan| null|  Algiers|null|  0|     3|    3|1995| 1995|          53.3|\n",
      "|Africa|       Japan| null|  Algiers|  25| 25|  null|    0|1995| 1995|          56.4|\n",
      "|Africa|       Japan| null|  Algiers|  26| 26|     3|    3|1995| 1995|          54.7|\n",
      "|Africa|       Japan| null|  Algiers|  27| 27|     3|    3|1995| 1995|          54.5|\n",
      "|Africa|       Japan| null|  Algiers|  28| 28|     3|    3|1995| 1995|          53.4|\n",
      "|Africa|       Japan| null|  Algiers|  29| 29|     3|    3|1995| 1995|          56.6|\n",
      "|Africa|       Japan| null|  Algiers|  30| 30|     3|    3|1995| 1995|          53.2|\n",
      "|Africa|       Japan| null|  Algiers|  31| 31|     3|    3|1995| 1995|          46.6|\n",
      "|Africa|       Japan| null|  Algiers|   1|  1|     4|    4|1995| 1995|          47.4|\n",
      "|Africa|       Japan| null|  Algiers|   2|  2|     4|    4|1995| 1995|          52.0|\n",
      "|Africa|       Japan| null|  Algiers|   3|  3|     4|    4|1995| 1995|          62.2|\n",
      "|Africa|       Japan| null|  Algiers|   4|  4|     4|    4|1995| 1995|          64.2|\n",
      "|Africa|       Japan| null|  Algiers|   5|  5|     4|    4|1995| 1995|          59.5|\n",
      "|Africa|       Japan| null|  Algiers|   6|  6|     4|    4|1995| 1995|          59.0|\n",
      "|Africa|       Japan| null|  Algiers|   7|  7|     4|    4|1995| 1995|          54.9|\n",
      "+------+------------+-----+---------+----+---+------+-----+----+-----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_temperature1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ecab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=spark.sql(\"\"\"\n",
    "select Region, temp_country, State,city_temp, CONCAT(Day1, '/',  Month1, '/',Year1) as date_temp,\n",
    "day, month,year,\n",
    "AvgTemperature  from city_temperature1 \"\"\")\n",
    "broadcast_df=df3.join(broadcast(df2),df3.temp_country==df2.country).cache()\n",
    "\n",
    "#remove null field date fields\n",
    "\n",
    "# broadcast_df.na.drop(Seq(\"date_temp\")).show(false)\n",
    "broadcast_df=broadcast_df.where(broadcast_df.date_temp.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7ee6509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+\n",
      "|Region|temp_country|State|city_temp|date_temp|day|month|year|AvgTemperature|   city|city_ascii|    lat|     lng|country|iso2|iso3|admin_name|capital|population|        id|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+\n",
      "|Africa|       Japan| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers|26/3/1995| 26|    3|1995|          54.7|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers|27/3/1995| 27|    3|1995|          54.5|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers|28/3/1995| 28|    3|1995|          53.4|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers|29/3/1995| 29|    3|1995|          56.6|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers|30/3/1995| 30|    3|1995|          53.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers|31/3/1995| 31|    3|1995|          46.6|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers| 1/4/1995|  1|    4|1995|          47.4|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers| 2/4/1995|  2|    4|1995|          52.0|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers| 3/4/1995|  3|    4|1995|          62.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers| 4/4/1995|  4|    4|1995|          64.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers| 5/4/1995|  5|    4|1995|          59.5|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers| 6/4/1995|  6|    4|1995|          59.0|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers| 7/4/1995|  7|    4|1995|          54.9|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers| 8/4/1995|  8|    4|1995|          54.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers|9/12/1995|  9|   12|1995|          57.8|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "|Africa|       Japan| null|  Algiers|15/4/1995| 15|    4|1995|          54.5|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broadcast_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce8279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Region: string (nullable = true)\n",
      " |-- temp_country: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- city_temp: string (nullable = true)\n",
      " |-- date_temp: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- AvgTemperature: float (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- city_ascii: string (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lng: float (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- iso2: string (nullable = true)\n",
      " |-- iso3: string (nullable = true)\n",
      " |-- admin_name: string (nullable = true)\n",
      " |-- capital: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date_new: date (nullable = true)\n",
      "\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+\n",
      "|Region|temp_country|State|city_temp|date_temp|day|month|year|AvgTemperature|   city|city_ascii|    lat|     lng|country|iso2|iso3|admin_name|capital|population|        id|  date_new|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+\n",
      "|Africa|       Japan| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-01-01|\n",
      "|Africa|       Japan| null|  Algiers|26/3/1995| 26|    3|1995|          54.7|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-26|\n",
      "|Africa|       Japan| null|  Algiers|27/3/1995| 27|    3|1995|          54.5|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-27|\n",
      "|Africa|       Japan| null|  Algiers|28/3/1995| 28|    3|1995|          53.4|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-28|\n",
      "|Africa|       Japan| null|  Algiers|29/3/1995| 29|    3|1995|          56.6|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-29|\n",
      "|Africa|       Japan| null|  Algiers|30/3/1995| 30|    3|1995|          53.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-30|\n",
      "|Africa|       Japan| null|  Algiers|31/3/1995| 31|    3|1995|          46.6|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-31|\n",
      "|Africa|       Japan| null|  Algiers| 1/4/1995|  1|    4|1995|          47.4|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-01|\n",
      "|Africa|       Japan| null|  Algiers| 2/4/1995|  2|    4|1995|          52.0|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-02|\n",
      "|Africa|       Japan| null|  Algiers| 3/4/1995|  3|    4|1995|          62.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-03|\n",
      "|Africa|       Japan| null|  Algiers| 4/4/1995|  4|    4|1995|          64.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-04|\n",
      "|Africa|       Japan| null|  Algiers| 5/4/1995|  5|    4|1995|          59.5|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-05|\n",
      "|Africa|       Japan| null|  Algiers| 6/4/1995|  6|    4|1995|          59.0|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-06|\n",
      "|Africa|       Japan| null|  Algiers| 7/4/1995|  7|    4|1995|          54.9|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-07|\n",
      "|Africa|       Japan| null|  Algiers| 8/4/1995|  8|    4|1995|          54.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-08|\n",
      "|Africa|       Japan| null|  Algiers|9/12/1995|  9|   12|1995|          57.8|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-12-09|\n",
      "|Africa|       Japan| null|  Algiers|15/4/1995| 15|    4|1995|          54.5|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-15|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "func =  udf (lambda x: datetime.strptime(x, '%d/%m/%Y'), DateType())\n",
    "\n",
    "broadcast_df1 = broadcast_df.withColumn('date_new', func(col('date_temp')))\n",
    "broadcast_df1.printSchema()\n",
    "broadcast_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca558b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Region: string (nullable = true)\n",
      " |-- temp_country: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- city_temp: string (nullable = true)\n",
      " |-- date_temp: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- AvgTemperature: float (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- city_ascii: string (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lng: float (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- iso2: string (nullable = true)\n",
      " |-- admin_name: string (nullable = true)\n",
      " |-- capital: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date_new: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Region: string (nullable = true)\n",
      " |-- temp_country: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- city_temp: string (nullable = true)\n",
      " |-- date_temp: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- AvgTemperature: float (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- city_ascii: string (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lng: float (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- iso3: string (nullable = true)\n",
      " |-- admin_name: string (nullable = true)\n",
      " |-- capital: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date_new: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Region: string (nullable = true)\n",
      " |-- temp_country: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- city_temp: string (nullable = true)\n",
      " |-- date_temp: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- AvgTemperature: float (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- city_ascii: string (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lng: float (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- iso3: string (nullable = true)\n",
      " |-- admin_name: string (nullable = true)\n",
      " |-- capital: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date_new: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##dropping a column from the dataframe different ways\n",
    "# pyspark.sql.functions import col\n",
    "\n",
    "broadcast_df1.drop(\"iso3\").printSchema()\n",
    "\n",
    "broadcast_df1.drop(col(\"iso2\")) \\\n",
    "  .printSchema()  \n",
    "  \n",
    "broadcast_df1.drop(broadcast_df1.iso2) \\\n",
    "  .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa2297bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Region: string (nullable = true)\n",
      " |-- temp_country: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- city_temp: string (nullable = true)\n",
      " |-- date_temp: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- AvgTemperature: float (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- city_ascii: string (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lng: float (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- admin_name: string (nullable = true)\n",
      " |-- capital: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date_new: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Region: string (nullable = true)\n",
      " |-- temp_country: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- city_temp: string (nullable = true)\n",
      " |-- date_temp: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- AvgTemperature: float (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- city_ascii: string (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lng: float (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- admin_name: string (nullable = true)\n",
      " |-- capital: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date_new: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping more than 1 columns\n",
    "broadcast_df1.drop(\"iso3\",\"iso2\") \\\n",
    "    .printSchema()\n",
    "\n",
    "cols = (\"iso3\",\"iso2\")\n",
    "\n",
    "broadcast_df1.drop(*cols) \\\n",
    "   .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66405df",
   "metadata": {},
   "outputs": [],
   "source": [
    "##PySpark splits the records based on the partition column and stores each partition data into a sub-directory.\n",
    "broadcast_df1.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"year\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"/home/rahul/Interview/Weatherforcast/broadcast_df1.1.csv/year/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ec6f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##PySpark splits the records based on the partition column and stores each partition data into a sub-directory.\n",
    "broadcast_df1.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"year\",\"temp_country\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"/home/rahul/Interview/Weatherforcast/broadcast_df1.1.csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95784b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use repartition() and partitionBy() together\n",
    "broadcast_df1.repartition(2) \\\n",
    "        .write.option(\"header\",True) \\\n",
    "        .partitionBy(\"year\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"/home/rahul/Interview/Weatherforcast/broadcast_df1.rep.csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61a356c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partitionBy() control number of partitions\n",
    "broadcast_df1.write.option(\"header\",True) \\\n",
    "        .option(\"maxRecordsPerFile\", 2) \\\n",
    "        .partitionBy(\"year\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"/home/rahul/Interview/Weatherforcast/broadcast_df1.rep.csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c245341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|year|            Japan|\n",
      "+----+-----------------+\n",
      "|1995|55.81764692418716|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pivoting of country\n",
    "broadcast_df1.groupBy(\"year\").pivot(\"Country\").avg(\"AvgTemperature\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72ffcbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:================================================>      (89 + 1) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+\n",
      "|Region|temp_country|State|city_temp|date_temp|day|month|year|AvgTemperature|   city|city_ascii|    lat|     lng|country|iso2|iso3|admin_name|capital|population|        id|  date_new|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+\n",
      "|Africa|       Japan| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-01-01|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+----+----+\n",
      "|country|              avg|               sum| min| max|\n",
      "+-------+-----------------+------------------+----+----+\n",
      "|  Japan|64.19999694824219|128.39999389648438|64.2|64.2|\n",
      "+-------+-----------------+------------------+----+----+\n",
      "\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+\n",
      "|Region|temp_country|State|city_temp|date_temp|day|month|year|AvgTemperature|   city|city_ascii|    lat|     lng|country|iso2|iso3|admin_name|capital|population|        id|  date_new|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+\n",
      "|Africa|       Japan| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-01-01|\n",
      "|Africa|       Japan| null|  Algiers|26/3/1995| 26|    3|1995|          54.7|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-26|\n",
      "|Africa|       Japan| null|  Algiers|27/3/1995| 27|    3|1995|          54.5|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-27|\n",
      "|Africa|       Japan| null|  Algiers|28/3/1995| 28|    3|1995|          53.4|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-28|\n",
      "|Africa|       Japan| null|  Algiers|29/3/1995| 29|    3|1995|          56.6|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-29|\n",
      "|Africa|       Japan| null|  Algiers|30/3/1995| 30|    3|1995|          53.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-30|\n",
      "|Africa|       Japan| null|  Algiers|31/3/1995| 31|    3|1995|          46.6|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-31|\n",
      "|Africa|       Japan| null|  Algiers| 1/4/1995|  1|    4|1995|          47.4|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-01|\n",
      "|Africa|       Japan| null|  Algiers| 2/4/1995|  2|    4|1995|          52.0|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-02|\n",
      "|Africa|       Japan| null|  Algiers| 3/4/1995|  3|    4|1995|          62.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-03|\n",
      "|Africa|       Japan| null|  Algiers| 4/4/1995|  4|    4|1995|          64.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-04|\n",
      "|Africa|       Japan| null|  Algiers| 5/4/1995|  5|    4|1995|          59.5|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-05|\n",
      "|Africa|       Japan| null|  Algiers| 6/4/1995|  6|    4|1995|          59.0|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-06|\n",
      "|Africa|       Japan| null|  Algiers| 7/4/1995|  7|    4|1995|          54.9|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-07|\n",
      "|Africa|       Japan| null|  Algiers| 8/4/1995|  8|    4|1995|          54.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-08|\n",
      "|Africa|       Japan| null|  Algiers|9/12/1995|  9|   12|1995|          57.8|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-12-09|\n",
      "|Africa|       Japan| null|  Algiers|15/4/1995| 15|    4|1995|          54.5|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-15|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+\n",
      "\n",
      "+----+-------------------+----------+\n",
      "|code|            country|contry3str|\n",
      "+----+-------------------+----------+\n",
      "|   4|        Afghanistan|       AFG|\n",
      "| 248|      Aland Islands|       ALA|\n",
      "|   8|            Albania|       ALB|\n",
      "|  12|            Algeria|       DZA|\n",
      "|  16|     American Samoa|       ASM|\n",
      "|  20|            Andorra|       AND|\n",
      "|  24|             Angola|       AGO|\n",
      "| 660|           Anguilla|       AIA|\n",
      "|  28|Antigua and Barbuda|       ATG|\n",
      "|  32|          Argentina|       ARG|\n",
      "|  51|            Armenia|       ARM|\n",
      "| 533|              Aruba|       ABW|\n",
      "|  36|          Australia|       AUS|\n",
      "|  40|            Austria|       AUT|\n",
      "|  31|         Azerbaijan|       AZE|\n",
      "|  44|            Bahamas|       BHS|\n",
      "|  48|            Bahrain|       BHR|\n",
      "|  50|         Bangladesh|       BGD|\n",
      "|  52|           Barbados|       BRB|\n",
      "| 112|            Belarus|       BLR|\n",
      "+----+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+-------+----------+\n",
      "|code|country|contry3str|\n",
      "+----+-------+----------+\n",
      "| 533|  Aruba|       ABW|\n",
      "+----+-------+----------+\n",
      "\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+\n",
      "|Region|temp_country|State|city_temp|date_temp|day|month|year|AvgTemperature|   city|city_ascii|    lat|     lng|country|iso2|iso3|admin_name|capital|population|        id|  date_new|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+\n",
      "|Africa|       Japan| null|  Algiers|15/4/1995| 15|    4|1995|          54.5|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-15|\n",
      "|Africa|       Japan| null|  Algiers|9/12/1995|  9|   12|1995|          57.8|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-12-09|\n",
      "|Africa|       Japan| null|  Algiers| 8/4/1995|  8|    4|1995|          54.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-08|\n",
      "|Africa|       Japan| null|  Algiers| 7/4/1995|  7|    4|1995|          54.9|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-07|\n",
      "|Africa|       Japan| null|  Algiers| 6/4/1995|  6|    4|1995|          59.0|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-06|\n",
      "|Africa|       Japan| null|  Algiers| 5/4/1995|  5|    4|1995|          59.5|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-05|\n",
      "|Africa|       Japan| null|  Algiers| 4/4/1995|  4|    4|1995|          64.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-04|\n",
      "|Africa|       Japan| null|  Algiers| 3/4/1995|  3|    4|1995|          62.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-03|\n",
      "|Africa|       Japan| null|  Algiers| 2/4/1995|  2|    4|1995|          52.0|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-02|\n",
      "|Africa|       Japan| null|  Algiers| 1/4/1995|  1|    4|1995|          47.4|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-04-01|\n",
      "|Africa|       Japan| null|  Algiers|31/3/1995| 31|    3|1995|          46.6|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-31|\n",
      "|Africa|       Japan| null|  Algiers|30/3/1995| 30|    3|1995|          53.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-30|\n",
      "|Africa|       Japan| null|  Algiers|29/3/1995| 29|    3|1995|          56.6|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-29|\n",
      "|Africa|       Japan| null|  Algiers|28/3/1995| 28|    3|1995|          53.4|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-28|\n",
      "|Africa|       Japan| null|  Algiers|27/3/1995| 27|    3|1995|          54.5|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-27|\n",
      "|Africa|       Japan| null|  Algiers|26/3/1995| 26|    3|1995|          54.7|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-03-26|\n",
      "|Africa|       Japan| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-01-01|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+\n",
      "\n",
      "+------------+\n",
      "|no_of_cities|\n",
      "+------------+\n",
      "|          17|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#highest temperature  based on countries\n",
    "w3 = Window.partitionBy(\"country\").orderBy(col(\"AvgTemperature\").desc())\n",
    "broadcast_df1.withColumn(\"row\",row_number().over(w3))   .filter(col(\"row\") == 1).drop(\"row\")   .show()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "broadcast_df1.withColumn(\"row\",row_number().over(w3))   .withColumn(\"avg\", avg(col(\"AvgTemperature\")).over(w3))   .withColumn(\"sum\", sum(col(\"AvgTemperature\")).over(w3))   .withColumn(\"min\", min(col(\"AvgTemperature\")).over(w3))   .withColumn(\"max\", max(col(\"AvgTemperature\")).over(w3))   .where(col(\"row\")==1).select(\"country\",\"avg\",\"sum\",\"min\",\"max\")   .show()\n",
    "\n",
    "\n",
    "# In[75]:\n",
    "\n",
    "\n",
    "\n",
    "broadcast_df1.createOrReplaceTempView(\"broadcast_df1\")\n",
    "broadcast_df1.cache()\n",
    "broadcast_df1.show()\n",
    "\n",
    "\n",
    "# In[98]:\n",
    "\n",
    "\n",
    "country_lkp = spark.read.option(\"header\", \"true\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").csv(\"/home/rahul/Interview/Weatherforcast/country_lkp.csv\")\n",
    "country_lkp.createOrReplaceTempView(\"country_lkp\")\n",
    "country_lkp.cache() \n",
    "country_lkp.show()\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select * from country_lkp where UPPER(country) like '%ARUBA%'\"\"\").show()\n",
    "\n",
    "\n",
    "# In[100]:\n",
    "\n",
    "\n",
    "broadcast_df2=spark.sql(\"\"\"\n",
    "select a.* from broadcast_df1 a inner join country_lkp b on upper(a.country)=upper(b.country) \"\"\")\n",
    "\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "\n",
    "\n",
    "# broadcast_df2 = broadcast_df1.merge(country_lkp, left_on=broadcast_df1[\"country\"].str.lower(), right_on=[\"country\"].str.lower(), how=\"left\")\n",
    "\n",
    "broadcast_df2.createOrReplaceTempView(\"broadcast_df2\")\n",
    "broadcast_df2.cache()\n",
    "broadcast_df2.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #Total number of countries\n",
    "no_of_cities=spark.sql(\"\"\"select count(city_temp) as no_of_cities from broadcast_df2\"\"\")\n",
    "no_of_cities.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac151c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|no_of_countries|\n",
      "+---------------+\n",
      "|             17|\n",
      "+---------------+\n",
      "\n",
      "+-----------------+\n",
      "|     avg_wrldtemp|\n",
      "+-----------------+\n",
      "|55.81764692418716|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|    avg_cntrytemp|\n",
      "+-----------------+\n",
      "|55.81764692418716|\n",
      "+-----------------+\n",
      "\n",
      "+---------+-----------------+\n",
      "|countries|    avg_cntrytemp|\n",
      "+---------+-----------------+\n",
      "|    Japan|55.81764692418716|\n",
      "+---------+-----------------+\n",
      "\n",
      "+---------+-------------+\n",
      "|countries|avg_cntrytemp|\n",
      "+---------+-------------+\n",
      "+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# #Total number of countries\n",
    "no_of_countries=spark.sql(\"\"\"select count(temp_country) as no_of_countries from broadcast_df2\"\"\")\n",
    "no_of_countries.cache()\n",
    "no_of_countries.show()\n",
    "\n",
    "\n",
    "# #AVg temp of around the world\n",
    "avg_wrldtemp=spark.sql(\"\"\"select avg(AvgTemperature) as avg_wrldtemp from broadcast_df2\"\"\")\n",
    "avg_wrldtemp.cache()\n",
    "avg_wrldtemp.show()\n",
    "\n",
    "\n",
    "# #AVg temp of around the world\n",
    "avg_cntrytemp=spark.sql(\"\"\"select avg(AvgTemperature) as avg_cntrytemp from broadcast_df2 group by temp_country\"\"\")\n",
    "avg_cntrytemp.cache()\n",
    "avg_cntrytemp.show()\n",
    "\n",
    "# #AVg temp of around the world between year 1995 and 2000\n",
    "avg_cntrytemp_1995_2000=spark.sql(\"\"\"select temp_country as countries,avg(AvgTemperature) as avg_cntrytemp from broadcast_df2 where year between 1995 and 2000 group by temp_country\"\"\")\n",
    "avg_cntrytemp_1995_2000.cache()\n",
    "avg_cntrytemp_1995_2000.show()\n",
    "\n",
    "# #AVg temp of around the world between year 1995 and 2000\n",
    "avg_cntrytemp_2000_2010=spark.sql(\"\"\"select temp_country as countries, avg(AvgTemperature) as avg_cntrytemp from broadcast_df2 where year between 2000 and 2010 group by temp_country\"\"\")\n",
    "avg_cntrytemp_2000_2010.cache()\n",
    "avg_cntrytemp_2000_2010.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db498c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----+---------+---------+---+-----+----+--------------+----+----------+---+---+-------+----+----+----------+-------+----------+---+--------+\n",
      "|Region|temp_country|State|city_temp|date_temp|day|month|year|AvgTemperature|city|city_ascii|lat|lng|country|iso2|iso3|admin_name|capital|population| id|date_new|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+----+----------+---+---+-------+----+----+----------+-------+----------+---+--------+\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+----+----------+---+---+-------+----+----+----------+-------+----------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #AVg temp of around the world between year 1995 and 2000\n",
    "avg_cntrytemp_2000_2010=spark.sql(\"\"\"select * from broadcast_df2 where year between 2000 and 2010 limit 10\"\"\")\n",
    "avg_cntrytemp_2000_2010.cache()\n",
    "avg_cntrytemp_2000_2010.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb6f925f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+---------+\n",
      "|Region|temp_country|State|city_temp|date_temp|day|month|year|AvgTemperature|   city|city_ascii|    lat|     lng|country|iso2|iso3|admin_name|capital|population|        id|  date_new|RowNumber|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+---------+\n",
      "|Africa|       Japan| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|Tokyo12|     Tokyo|35.6897|139.6922|  Japan| JP1| JPN|     Tōkyō|primary|  37977000|1392685764|1995-01-01|        1|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+-------+----------+-------+--------+-------+----+----+----------+-------+----------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_distinct_state=spark.sql(\"\"\"SELECT  *\n",
    "FROM    (SELECT *,\n",
    "                ROW_NUMBER() OVER (PARTITION BY state ORDER BY date_temp) AS RowNumber\n",
    "         FROM   broadcast_df2\n",
    "         ) AS a\n",
    "WHERE   a.RowNumber = 1\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce7413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_original_1bil_records= spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/home/rahul/Interview/Weatherforcast/broadcast_df.csv/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "100adc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104049799"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_original_1bil_records.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59df2930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----+---------+---------+---+-----+----+--------------+---------------+---------------+-------+------+-------+----+----+------------------+-------+----------+----------+\n",
      "|Region|temp_country|State|city_temp|date_temp|day|month|year|AvgTemperature|           city|     city_ascii|    lat|   lng|country|iso2|iso3|        admin_name|capital|population|        id|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+---------------+---------------+-------+------+-------+----+----+------------------+-------+----------+----------+\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|     I-n-Amguel|     I-n-Amguel|23.6936|5.1647|Algeria|  DZ| DZA|       Tamanrasset|   null|      3030|1012162135|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|      Tassamert|      Tassamert|36.2693| 4.823|Algeria|  DZ| DZA|Bordj Bou Arréridj|   null|      5269|1012240413|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|     Beni Ounif|     Beni Ounif|  32.05| -1.25|Algeria|  DZ| DZA|            Béchar|   null|      5628|1012327989|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|        El Maïn|        El Main|36.3667|  4.75|Algeria|  DZ| DZA|            Bejaïa|   null|      6238|1012706441|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|  Aït Bou Madhi|  Ait Bou Madhi|36.5009|   4.2|Algeria|  DZ| DZA|        Tizi Ouzou|   null|      6113|1012471822|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|     In Guezzam|     In Guezzam|19.5686|5.7722|Algeria|  DZ| DZA|       Tamanrasset|   null|      7045|1012000031|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|    Souk el Had|    Souk el Had|36.6909|3.5889|Algeria|  DZ| DZA|         Boumerdes|   null|      6693|1012532288|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|     Beni Aïssi|     Beni Aissi|   29.6|  0.25|Algeria|  DZ| DZA|             Adrar|   null|      7628|1012574444|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|        Taourga|        Taourga|36.7938|3.9463|Algeria|  DZ| DZA|         Boumerdes|   null|      7882|1012841664|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|         Amalou|         Amalou|36.4778|4.6333|Algeria|  DZ| DZA|            Bejaïa|   null|      8602|1012045463|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|          Tifra|          Tifra|36.6664|4.6972|Algeria|  DZ| DZA|            Bejaïa|   null|      8547|1012591585|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|     Boukhralfa|     Boukhralfa|36.6144|5.0872|Algeria|  DZ| DZA|            Bejaïa|   null|      8766|1012066745|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|       Abalessa|       Abalessa|  22.89|4.8472|Algeria|  DZ| DZA|       Tamanrasset|   null|      9163|1012479386|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|     I-n-Amenas|     I-n-Amenas|  28.05|  9.55|Algeria|  DZ| DZA|            Illizi|   null|      9225|1012221429|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|Seddouk Oufella|Seddouk Oufella|36.6061|4.6389|Algeria|  DZ| DZA|            Bejaïa|   null|      8931|1012348181|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|         Ifigha|         Ifigha|36.6667|4.4167|Algeria|  DZ| DZA|        Tizi Ouzou|   null|      9160|1012938624|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|   Ouled Rached|   Ouled Rached|36.2119|4.1106|Algeria|  DZ| DZA|            Bouira|   null|      9311|1012727170|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|         Toudja|         Toudja|36.7586|4.8933|Algeria|  DZ| DZA|            Bejaïa|   null|      9827|1012114797|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|       Bou Nouh|       Bou Nouh|   36.5|3.9333|Algeria|  DZ| DZA|        Tizi Ouzou|   null|      9731|1012718665|\n",
      "|Africa|     Algeria| null|  Algiers| 1/1/1995|  1|    1|1995|          64.2|         Souama|         Souama|36.6417|4.3416|Algeria|  DZ| DZA|        Tizi Ouzou|   null|      9954|1012240654|\n",
      "+------+------------+-----+---------+---------+---+-----+----+--------------+---------------+---------------+-------+------+-------+----+----+------------------+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "distinct_original_1bil_records.cache()\n",
    "distinct_original_1bil_records.show()\n",
    "distinct_original_1bil_records.createOrReplaceTempView(\"distinct_original_1bil_records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7de8d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rahul/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rahul/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/rahul/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o22.sql",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4830/3877713699.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m data_distinct_state=spark.sql(\"\"\"SELECT  *\n\u001b[0m\u001b[1;32m      2\u001b[0m FROM    (SELECT *,\n\u001b[1;32m      3\u001b[0m                 \u001b[0mROW_NUMBER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mOVER\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPARTITION\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mstate\u001b[0m \u001b[0mORDER\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mdate_temp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mAS\u001b[0m \u001b[0mRowNumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m          \u001b[0mFROM\u001b[0m   \u001b[0mdistinct_original_1bil_records\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtemp_country\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Algeria'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m          ) AS a\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise Py4JError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 format(target_id, \".\", name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o22.sql"
     ]
    }
   ],
   "source": [
    "data_distinct_state=spark.sql(\"\"\"SELECT  *\n",
    "FROM    (SELECT *,\n",
    "                ROW_NUMBER() OVER (PARTITION BY state ORDER BY date_temp) AS RowNumber\n",
    "         FROM   distinct_original_1bil_records where temp_country='Algeria'\n",
    "         ) AS a\n",
    "WHERE   a.RowNumber = 1\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab8fcd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_distinct_state=spark.sql(\"\"\"SELECT  *\n",
    "FROM (SELECT *,\n",
    "                ROW_NUMBER() OVER (PARTITION BY temp_country ORDER BY date_temp) AS RowNumber\n",
    "         FROM   distinct_original_1bil_records ) AS a\n",
    "         where a.RowNumber =1 limit 10\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "960036f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/25 00:44:37 WARN MemoryStore: Not enough space to cache rdd_13_18 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:38 WARN MemoryStore: Not enough space to cache rdd_13_19 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:39 WARN MemoryStore: Not enough space to cache rdd_13_20 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:41 WARN MemoryStore: Not enough space to cache rdd_13_21 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:42 WARN MemoryStore: Not enough space to cache rdd_13_22 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:43 WARN MemoryStore: Not enough space to cache rdd_13_23 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:45 WARN MemoryStore: Not enough space to cache rdd_13_24 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:46 WARN MemoryStore: Not enough space to cache rdd_13_25 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:47 WARN MemoryStore: Not enough space to cache rdd_13_26 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:48 WARN MemoryStore: Not enough space to cache rdd_13_27 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:50 WARN MemoryStore: Not enough space to cache rdd_13_28 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:51 WARN MemoryStore: Not enough space to cache rdd_13_29 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:52 WARN MemoryStore: Not enough space to cache rdd_13_30 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:54 WARN MemoryStore: Not enough space to cache rdd_13_31 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:55 WARN MemoryStore: Not enough space to cache rdd_13_32 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:56 WARN MemoryStore: Not enough space to cache rdd_13_33 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:58 WARN MemoryStore: Not enough space to cache rdd_13_34 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:44:59 WARN MemoryStore: Not enough space to cache rdd_13_35 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:00 WARN MemoryStore: Not enough space to cache rdd_13_36 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:01 WARN MemoryStore: Not enough space to cache rdd_13_37 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:03 WARN MemoryStore: Not enough space to cache rdd_13_38 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:04 WARN MemoryStore: Not enough space to cache rdd_13_39 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:05 WARN MemoryStore: Not enough space to cache rdd_13_40 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:07 WARN MemoryStore: Not enough space to cache rdd_13_41 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:08 WARN MemoryStore: Not enough space to cache rdd_13_42 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:09 WARN MemoryStore: Not enough space to cache rdd_13_43 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:10 WARN MemoryStore: Not enough space to cache rdd_13_44 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:12 WARN MemoryStore: Not enough space to cache rdd_13_45 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:13 WARN MemoryStore: Not enough space to cache rdd_13_46 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:14 WARN MemoryStore: Not enough space to cache rdd_13_47 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:16 WARN MemoryStore: Not enough space to cache rdd_13_48 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:17 WARN MemoryStore: Not enough space to cache rdd_13_49 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:18 WARN MemoryStore: Not enough space to cache rdd_13_50 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:19 WARN MemoryStore: Not enough space to cache rdd_13_51 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:21 WARN MemoryStore: Not enough space to cache rdd_13_52 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:22 WARN MemoryStore: Not enough space to cache rdd_13_53 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:23 WARN MemoryStore: Not enough space to cache rdd_13_54 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:24 WARN MemoryStore: Not enough space to cache rdd_13_55 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:26 WARN MemoryStore: Not enough space to cache rdd_13_56 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:27 WARN MemoryStore: Not enough space to cache rdd_13_57 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:28 WARN MemoryStore: Not enough space to cache rdd_13_58 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:30 WARN MemoryStore: Not enough space to cache rdd_13_59 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:31 WARN MemoryStore: Not enough space to cache rdd_13_60 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:32 WARN MemoryStore: Not enough space to cache rdd_13_61 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:33 WARN MemoryStore: Not enough space to cache rdd_13_62 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:35 WARN MemoryStore: Not enough space to cache rdd_13_63 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:36 WARN MemoryStore: Not enough space to cache rdd_13_64 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:37 WARN MemoryStore: Not enough space to cache rdd_13_65 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:38 WARN MemoryStore: Not enough space to cache rdd_13_66 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:40 WARN MemoryStore: Not enough space to cache rdd_13_67 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:41 WARN MemoryStore: Not enough space to cache rdd_13_68 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:42 WARN MemoryStore: Not enough space to cache rdd_13_69 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:44 WARN MemoryStore: Not enough space to cache rdd_13_70 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:45 WARN MemoryStore: Not enough space to cache rdd_13_71 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:46 WARN MemoryStore: Not enough space to cache rdd_13_72 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:47 WARN MemoryStore: Not enough space to cache rdd_13_73 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:49 WARN MemoryStore: Not enough space to cache rdd_13_74 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:50 WARN MemoryStore: Not enough space to cache rdd_13_75 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:51 WARN MemoryStore: Not enough space to cache rdd_13_76 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:53 WARN MemoryStore: Not enough space to cache rdd_13_77 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:54 WARN MemoryStore: Not enough space to cache rdd_13_78 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:55 WARN MemoryStore: Not enough space to cache rdd_13_79 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:57 WARN MemoryStore: Not enough space to cache rdd_13_80 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:58 WARN MemoryStore: Not enough space to cache rdd_13_81 in memory! (computed 5.1 MiB so far)\n",
      "22/01/25 00:45:59 WARN MemoryStore: Not enough space to cache rdd_13_82 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:00 WARN MemoryStore: Not enough space to cache rdd_13_83 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:02 WARN MemoryStore: Not enough space to cache rdd_13_84 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:03 WARN MemoryStore: Not enough space to cache rdd_13_85 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:05 WARN MemoryStore: Not enough space to cache rdd_13_86 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:06 WARN MemoryStore: Not enough space to cache rdd_13_87 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:08 WARN MemoryStore: Not enough space to cache rdd_13_88 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:09 WARN MemoryStore: Not enough space to cache rdd_13_89 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:10 WARN MemoryStore: Not enough space to cache rdd_13_90 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:11 WARN MemoryStore: Not enough space to cache rdd_13_91 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:12 WARN MemoryStore: Not enough space to cache rdd_13_92 in memory! (computed 4.1 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/25 00:46:14 WARN MemoryStore: Not enough space to cache rdd_13_93 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:15 WARN MemoryStore: Not enough space to cache rdd_13_94 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:16 WARN MemoryStore: Not enough space to cache rdd_13_95 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:17 WARN MemoryStore: Not enough space to cache rdd_13_96 in memory! (computed 4.1 MiB so far)\n",
      "22/01/25 00:46:19 WARN MemoryStore: Not enough space to cache rdd_13_97 in memory! (computed 4.1 MiB so far)\n",
      "[Stage 17:=>                                                      (5 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1322.770s][warning][gc,alloc] Executor task launch worker for task 59.0 in stage 17.0 (TID 930): Retried waiting for GCLocker too often allocating 131074 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/25 00:49:48 ERROR Executor: Exception in task 59.0 in stage 17.0 (TID 930)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:133)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:142)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$2997/0x0000000841170840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2998/0x000000084116e440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2252/0x0000000840ebe440.apply(Unknown Source)\n",
      "22/01/25 00:49:48 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 59.0 in stage 17.0 (TID 930),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:133)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:142)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$2997/0x0000000841170840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2998/0x000000084116e440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2252/0x0000000840ebe440.apply(Unknown Source)\n",
      "22/01/25 00:49:48 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@2bbb578d rejected from java.util.concurrent.ThreadPoolExecutor@31fb6b3[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 930]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:270)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/01/25 00:49:48 WARN TaskSetManager: Lost task 59.0 in stage 17.0 (TID 930) (192.168.10.100 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:133)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:142)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$2997/0x0000000841170840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2998/0x000000084116e440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2252/0x0000000840ebe440.apply(Unknown Source)\n",
      "\n",
      "22/01/25 00:49:48 ERROR TaskSetManager: Task 59 in stage 17.0 failed 1 times; aborting job\n",
      "\r",
      "[Stage 17:=>                                                      (5 + 2) / 200]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o42.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 59 in stage 17.0 failed 1 times, most recent failure: Lost task 59.0 in stage 17.0 (TID 930) (192.168.10.100 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:133)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:142)\n\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:122)\n\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$2997/0x0000000841170840.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD$$Lambda$2998/0x000000084116e440.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2252/0x0000000840ebe440.apply(Unknown Source)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:133)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:142)\n\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:122)\n\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$2997/0x0000000841170840.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD$$Lambda$2998/0x000000084116e440.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2252/0x0000000840ebe440.apply(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10329/1799902950.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_distinct_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_distinct_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o42.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 59 in stage 17.0 failed 1 times, most recent failure: Lost task 59.0 in stage 17.0 (TID 930) (192.168.10.100 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:133)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:142)\n\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:122)\n\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$2997/0x0000000841170840.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD$$Lambda$2998/0x000000084116e440.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2252/0x0000000840ebe440.apply(Unknown Source)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:133)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:142)\n\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:122)\n\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$2997/0x0000000841170840.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD$$Lambda$2998/0x000000084116e440.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2252/0x0000000840ebe440.apply(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "data_distinct_state.cache()\n",
    "data_distinct_state.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d79664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
